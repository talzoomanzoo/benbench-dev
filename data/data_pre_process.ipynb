{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjgwak/miniconda3/envs/uid_2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 75.10ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 9.63kB / 9.63kB, 12.0kB/s  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 9.63kB / 9.63kB, 8.02kB/s  \n",
      "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
      "                                        : 100%|██████████| 9.63kB / 9.63kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.13s/ shards]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/brumo2025/commit/69ea201ce4a497bb3691701515aec8c652b118dc', commit_message='Upload dataset', commit_description='', oid='69ea201ce4a497bb3691701515aec8c652b118dc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/brumo2025', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/brumo2025'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./BRUMO\"):\n",
    "    os.makedirs(\"./BRUMO\")\n",
    "\n",
    "train_set = load_dataset(\"MathArena/brumo_2025\", split=\"train\")\n",
    "with open(\"./BRUMO/test.json\", \"w\") as f:\n",
    "    aime_train = []\n",
    "    for item in train_set:\n",
    "        aime_train.append({\n",
    "            'problem_idx': item['problem_idx'],\n",
    "            'Question': item['problem'],\n",
    "            'answer': item['answer'],\n",
    "            'problem_type': item['problem_type'],\n",
    "        })\n",
    "    json.dump(aime_train, f, ensure_ascii=False, indent=4)\n",
    "dataset = Dataset.from_list(aime_train)\n",
    "dataset.push_to_hub('talzoomanzoo/brumo2025', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all math datasets into one\n",
    "def compile_all_math_datasets():\n",
    "    data_dir = Path(\".\")  # Current directory\n",
    "    compiled_data = []\n",
    "    \n",
    "    # Define all dataset directories to process\n",
    "    datasets = {\n",
    "        \"AIME\": [\"train.json\", \"test.json\"],\n",
    "        \"BRUMO\": [\"test.json\"],\n",
    "        \"HMMT\": [\"test.json\"],\n",
    "        \"MINERVA_MATH\": [\"test.json\"],\n",
    "        \"OLYMPIADBENCH\": [\"test.json\"]\n",
    "    }\n",
    "    \n",
    "    for dataset_name, files in datasets.items():\n",
    "        dataset_path = data_dir / dataset_name\n",
    "        if not dataset_path.exists():\n",
    "            print(f\"Warning: {dataset_name} directory not found, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        for file_name in files:\n",
    "            file_path = dataset_path / file_name\n",
    "            if not file_path.exists():\n",
    "                print(f\"Warning: {file_path} not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing {dataset_name}/{file_name}...\")\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Extract Question and answer from each entry\n",
    "            for item in data:\n",
    "                # Handle different data structures\n",
    "                question = item.get(\"Question\", \"\")\n",
    "                answer = item.get(\"answer\", \"\")\n",
    "                \n",
    "                # Convert answer to string if it's a list\n",
    "                if isinstance(answer, list):\n",
    "                    answer = answer[0] if len(answer) > 0 else \"\"\n",
    "                \n",
    "                compiled_entry = {\n",
    "                    \"Question\": question,\n",
    "                    \"answer\": str(answer),\n",
    "                    \"source\": dataset_name,\n",
    "                    \"split\": file_name.replace(\".json\", \"\")\n",
    "                }\n",
    "                compiled_data.append(compiled_entry)\n",
    "    \n",
    "    print(f\"\\nTotal problems compiled: {len(compiled_data)}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset breakdown:\")\n",
    "    from collections import Counter\n",
    "    source_counts = Counter([item[\"source\"] for item in compiled_data])\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  {source}: {count} problems\")\n",
    "    \n",
    "    # Save compiled dataset\n",
    "    output_file = data_dir / \"compiled_math_dataset.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(compiled_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nCompiled dataset saved to: {output_file}\")\n",
    "    return compiled_data\n",
    "\n",
    "# Run the compilation\n",
    "compiled_dataset = compile_all_math_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 511.75ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████|  157kB /  157kB,  784kB/s  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████|  157kB /  157kB,  261kB/s  \n",
      "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
      "                                        : 100%|██████████|  157kB /  157kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.61s/ shards]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/olympiadbench/commit/58a71ac74e0cc78463bddb343bd98e0201ce90f1', commit_message='Upload dataset', commit_description='', oid='58a71ac74e0cc78463bddb343bd98e0201ce90f1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/olympiadbench', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/olympiadbench'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./OLYMPIADBENCH\"):\n",
    "    os.makedirs(\"./OLYMPIADBENCH\")\n",
    "\n",
    "test_set1 = load_dataset(\"Hothan/OlympiadBench\", 'OE_TO_maths_en_COMP', split=\"train\") #674\n",
    "test_set2 = load_dataset(\"Hothan/OlympiadBench\", 'OE_TO_maths_en_COMP', split=\"train\")\n",
    "\n",
    "with open(\"./OLYMPIADBENCH/test.json\", \"w\") as f:\n",
    "    aime_test = []\n",
    "    for item in test_set1:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['final_answer'],\n",
    "        })\n",
    "    for item in test_set2:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['final_answer'],\n",
    "        })\n",
    "\n",
    "    print(len(aime_test))\n",
    "    json.dump(aime_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(aime_test)\n",
    "dataset.push_to_hub('talzoomanzoo/olympiadbench', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 2403.61ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 8.01kB / 8.01kB, 40.1kB/s  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 8.01kB / 8.01kB, 13.4kB/s  \n",
      "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
      "                                        : 100%|██████████| 8.01kB / 8.01kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ shards]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/hmmt/commit/8602b342733ff6e33ab848936752a905478a5745', commit_message='Upload dataset', commit_description='', oid='8602b342733ff6e33ab848936752a905478a5745', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/hmmt', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/hmmt'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hmmt\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "#hmmt2025\n",
    "\n",
    "if not os.path.exists(\"./HMMT\"): \n",
    "    os.makedirs(\"./HMMT\")\n",
    "\n",
    "test_set = load_dataset(\"MathArena/hmmt_feb_2025\", split=\"train\")\n",
    "\n",
    "\n",
    "# Prepare test data\n",
    "hmmt_test = [] #30\n",
    "for item in test_set:\n",
    "    hmmt_test.append({\n",
    "        'Question': item['problem'],\n",
    "        'answer': item['answer'],\n",
    "    })\n",
    "\n",
    "# Save individual files\n",
    "with open(\"./HMMT/test.json\", \"w\") as f:\n",
    "    json.dump(hmmt_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Create DatasetDict with both splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'test': Dataset.from_list(hmmt_test)\n",
    "})\n",
    "\n",
    "# Upload to hub with both splits\n",
    "dataset_dict.push_to_hub('talzoomanzoo/hmmt', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1098.85ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 54.4kB / 54.4kB,  271kB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 54.4kB / 54.4kB,  136kB/s  \n",
      "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
      "                                        : 100%|██████████| 54.4kB / 54.4kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.41s/ shards]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/minervamath/commit/fc64b615c00e77b6c6bf9077815508779b5b7c04', commit_message='Upload dataset', commit_description='', oid='fc64b615c00e77b6c6bf9077815508779b5b7c04', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/minervamath', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/minervamath'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gsm8k\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./MINERVA_MATH\"):\n",
    "    os.makedirs(\"./MINERVA_MATH\")\n",
    "\n",
    "test_set = load_dataset(\"math-ai/minervamath\", split=\"test\")\n",
    "\n",
    "\n",
    "# Prepare test data\n",
    "minerva_math_test = [] #272\n",
    "for item in test_set:\n",
    "    minerva_math_test.append({\n",
    "        'Question': item['question'],\n",
    "        'answer': item['answer'],\n",
    "    })\n",
    "\n",
    "# Save individual files\n",
    "with open(\"./MINERVA_MATH/test.json\", \"w\") as f:\n",
    "    json.dump(minerva_math_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Create DatasetDict with both splits\n",
    "dataset_dict = DatasetDict({\n",
    "    'test': Dataset.from_list(minerva_math_test)\n",
    "})\n",
    "\n",
    "# Upload to hub with both splits\n",
    "dataset_dict.push_to_hub('talzoomanzoo/minervamath', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 458.34ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████|  177kB /  177kB,  886kB/s  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████|  177kB /  177kB,  295kB/s  \n",
      "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
      "                                        : 100%|██████████|  177kB /  177kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.61s/ shards]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/aime_train/commit/b8c17c666e0c1d0ac1e99dc7f8b95d599de81171', commit_message='Upload dataset', commit_description='', oid='b8c17c666e0c1d0ac1e99dc7f8b95d599de81171', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/aime_train', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/aime_train'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./AIME\"):\n",
    "    os.makedirs(\"./AIME\")\n",
    "\n",
    "train_set = load_dataset(\"gneubig/aime-1983-2024\", split=\"train\")\n",
    "test_set1 = load_dataset(\"opencompass/AIME2025\", 'AIME2025-I', split=\"test\")\n",
    "test_set2 = load_dataset(\"opencompass/AIME2025\", 'AIME2025-II', split=\"test\")\n",
    "with open(\"./AIME/train.json\", \"w\") as f:\n",
    "    aime_train = []\n",
    "    for item in train_set:\n",
    "        aime_train.append({\n",
    "            'id': item['ID'],\n",
    "            'year': item['Year'],\n",
    "            'problem_number': item['Problem Number'],\n",
    "            'Question': item['Question'],\n",
    "            'answer': item['Answer'],\n",
    "        })\n",
    "    json.dump(aime_train, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(\"./AIME/test.json\", \"w\") as f:\n",
    "    aime_test = []\n",
    "    for item in test_set1:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "    for item in test_set2:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "    json.dump(aime_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "dataset = Dataset.from_list(aime_train)\n",
    "dataset.push_to_hub('talzoomanzoo/aime_train', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 2293.22ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 9.27kB / 9.27kB, 23.1kB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 9.27kB / 9.27kB, 15.4kB/s  \n",
      "New Data Upload                         : 100%|██████████| 9.27kB / 9.27kB, 15.4kB/s  \n",
      "                                        : 100%|██████████| 9.27kB / 9.27kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.63s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/talzoomanzoo/aime_test/commit/ba97c97f2b7d494b381346b042071206eebf2c1b', commit_message='Upload dataset', commit_description='', oid='ba97c97f2b7d494b381346b042071206eebf2c1b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/talzoomanzoo/aime_test', endpoint='https://huggingface.co', repo_type='dataset', repo_id='talzoomanzoo/aime_test'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./AIME\"):\n",
    "    os.makedirs(\"./AIME\")\n",
    "\n",
    "test_set1 = load_dataset(\"opencompass/AIME2025\", 'AIME2025-I', split=\"test\")\n",
    "test_set2 = load_dataset(\"opencompass/AIME2025\", 'AIME2025-II', split=\"test\")\n",
    "\n",
    "with open(\"./AIME/test.json\", \"w\") as f:\n",
    "    aime_test = []\n",
    "    for item in test_set1:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "    for item in test_set2:\n",
    "        aime_test.append({\n",
    "            'Question': item['question'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "\n",
    "    print(len(aime_test))\n",
    "    json.dump(aime_test, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "dataset = Dataset.from_list(aime_test)\n",
    "dataset.push_to_hub('talzoomanzoo/aime_test', private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './GPQA/original_data/gpqa_diamond.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     15\u001b[0m keys_to_keep \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncorrect Answer 3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     26\u001b[0m filtered_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csv_file:\n\u001b[1;32m     28\u001b[0m     csv_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(csv_file)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(csv_reader), \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# Add id field\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/uid_2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './GPQA/original_data/gpqa_diamond.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocess for GPQA\n",
    "Data Link: https://huggingface.co/datasets/Idavidrein/gpqa\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths to data\n",
    "split = 'diamond'  # diamond, main, extended\n",
    "data_path = f'./GPQA/original_data/gpqa_{split}.csv'\n",
    "output_path = f'./GPQA/{split}.json'\n",
    "\n",
    "# Define the keys we want to keep\n",
    "keys_to_keep = [\n",
    "    'id',\n",
    "    'Question',\n",
    "    'Subdomain',\n",
    "    'High-level domain',\n",
    "    'Correct Answer',\n",
    "    'Incorrect Answer 1',\n",
    "    'Incorrect Answer 2',\n",
    "    'Incorrect Answer 3'\n",
    "]\n",
    "\n",
    "filtered_data = []\n",
    "with open(data_path, mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for idx, row in enumerate(tqdm(csv_reader), 0):\n",
    "        # Add id field\n",
    "        row['id'] = idx\n",
    "        # Create new dictionary with only desired keys\n",
    "        filtered_row = {key: row[key] for key in keys_to_keep}\n",
    "\n",
    "        # Extract answers and shuffle them\n",
    "        answers = [\n",
    "            ('Correct Answer', filtered_row['Correct Answer']),\n",
    "            ('Incorrect Answer 1', filtered_row['Incorrect Answer 1']),\n",
    "            ('Incorrect Answer 2', filtered_row['Incorrect Answer 2']),\n",
    "            ('Incorrect Answer 3', filtered_row['Incorrect Answer 3'])\n",
    "        ]\n",
    "        random.shuffle(answers)\n",
    "\n",
    "        # Assign new choices A, B, C, D in order and determine the correct choice\n",
    "        choices = ['A', 'B', 'C', 'D']\n",
    "        formatted_answers = []\n",
    "        correct_choice = None\n",
    "        for i, (label, answer) in enumerate(answers):\n",
    "            choice = choices[i]\n",
    "            formatted_answers.append((choice, answer))\n",
    "            if label == 'Correct Answer':\n",
    "                correct_choice = choice\n",
    "\n",
    "        # Update the Question field\n",
    "        formatted_choices = \"\\n\".join([f\"({choice}) {answer}\" for choice, answer in formatted_answers])\n",
    "        filtered_row['Question'] = f\"{filtered_row['Question']} Choices:\\n{formatted_choices}\\n\"\n",
    "\n",
    "        # Add the Correct Choice field\n",
    "        filtered_row['Correct Choice'] = correct_choice\n",
    "\n",
    "        # Append the updated row to filtered_data\n",
    "        filtered_data.append(filtered_row)\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(output_path, mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(filtered_data, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown split \"train\". Should be one of ['test'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./MATH500\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m test_set \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceH4/MATH-500\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHuggingFaceH4/MATH-500\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./MATH500/test.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     17\u001b[0m     math500_test \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py:2096\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2095\u001b[0m )\n\u001b[0;32m-> 2096\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_infos:\n\u001b[1;32m   2098\u001b[0m     builder_instance\u001b[38;5;241m.\u001b[39m_save_infos()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/builder.py:1127\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[1;32m   1124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS)\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[0;32m-> 1127\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_single_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_post_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_post_process\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1139\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m DatasetDict(datasets)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/utils/py_utils.py:494\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    493\u001b[0m     data_struct \u001b[38;5;241m=\u001b[39m [data_struct]\n\u001b[0;32m--> 494\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    496\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m mapped[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/builder.py:1157\u001b[0m, in \u001b[0;36mDatasetBuilder._build_single_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, in_memory)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     split \u001b[38;5;241m=\u001b[39m Split(split)\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# Build base dataset\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_post_process:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resource_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_processing_resources(split)\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/builder.py:1231\u001b[0m, in \u001b[0;36mDatasetBuilder._as_dataset\u001b[0;34m(self, split, in_memory)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_legacy_cache():\n\u001b[1;32m   1230\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m-> 1231\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mArrowReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1237\u001b[0m fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_fingerprint(split)\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(fingerprint\u001b[38;5;241m=\u001b[39mfingerprint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:248\u001b[0m, in \u001b[0;36mBaseReader.read\u001b[0;34m(self, name, instructions, split_infos, in_memory)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    229\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    233\u001b[0m ):\n\u001b[1;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns Dataset instance(s).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m         kwargs to build a single Dataset instance.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_file_instructions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files:\n\u001b[1;32m    250\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstruction \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m corresponds to no data!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:221\u001b[0m, in \u001b[0;36mBaseReader.get_file_instructions\u001b[0;34m(self, name, instruction, split_infos)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_file_instructions\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, instruction, split_infos):\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return list of dict {'filename': str, 'skip': int, 'take': int}\"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     file_instructions \u001b[38;5;241m=\u001b[39m \u001b[43mmake_file_instructions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiletype_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filetype_suffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     files \u001b[38;5;241m=\u001b[39m file_instructions\u001b[38;5;241m.\u001b[39mfile_instructions\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m files\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:130\u001b[0m, in \u001b[0;36mmake_file_instructions\u001b[0;34m(name, split_infos, instruction, filetype_suffix, prefix_path)\u001b[0m\n\u001b[1;32m    128\u001b[0m     instruction \u001b[38;5;241m=\u001b[39m ReadInstruction\u001b[38;5;241m.\u001b[39mfrom_spec(instruction)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Create the absolute instruction (per split)\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m absolute_instructions \u001b[38;5;241m=\u001b[39m \u001b[43minstruction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_absolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# For each split, return the files instruction (skip/take)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m file_instructions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:620\u001b[0m, in \u001b[0;36mReadInstruction.to_absolute\u001b[0;34m(self, name2len)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[1;32m    609\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [_rel_to_abs_instr(rel_instr, name2len) \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:620\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_absolute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name2len):\n\u001b[1;32m    609\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Translate instruction into a list of absolute instructions.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    Those absolute instructions are then to be added together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m        list of _AbsoluteInstruction instances (corresponds to the + in spec).\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_rel_to_abs_instr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_instr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname2len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m rel_instr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_relative_instructions]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/arrow_reader.py:437\u001b[0m, in \u001b[0;36m_rel_to_abs_instr\u001b[0;34m(rel_instr, name2len)\u001b[0m\n\u001b[1;32m    435\u001b[0m split \u001b[38;5;241m=\u001b[39m rel_instr\u001b[38;5;241m.\u001b[39msplitname\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name2len:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown split \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(name2len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    438\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m name2len[split]\n\u001b[1;32m    439\u001b[0m from_ \u001b[38;5;241m=\u001b[39m rel_instr\u001b[38;5;241m.\u001b[39mfrom_\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown split \"train\". Should be one of ['test']."
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocess for MATH500\n",
    "Data Link: https://huggingface.co/datasets/HuggingFaceH4/MATH-500\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "if not os.path.exists(\"./MATH500\"):\n",
    "    os.makedirs(\"./MATH500\")\n",
    "\n",
    "test_set = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")\n",
    "\n",
    "with open(\"./MATH500/test.json\", 'w') as file:\n",
    "    math500_test = []\n",
    "    for id, line in enumerate(tqdm(test_set)):\n",
    "        math500_test.append({\n",
    "            'id': id, \n",
    "            'Question': line['problem'],\n",
    "            'solution': line['solution'],\n",
    "            'answer': line['answer'],\n",
    "            'subject': line['subject'],\n",
    "            'level': line['level'],\n",
    "            'unique_id': line['unique_id'],\n",
    "        })\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(\"./MATH500/test.json\", mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(math500_test, json_file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [00:00<00:00, 43015.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data preprocess for AMC\n",
    "\"\"\"\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.exists(\"./AMC\"):\n",
    "    os.makedirs(\"./AMC\")\n",
    "\n",
    "test_set = load_dataset(\"AI-MO/aimo-validation-amc\", split=\"train\")\n",
    "\n",
    "data_list = []\n",
    "with open(\"./AMC/test.json\", 'w') as file:\n",
    "    amc_test = []\n",
    "    for id, line in enumerate(tqdm(test_set)):\n",
    "        amc_test.append({\n",
    "            'id': id, \n",
    "            'Question': line['problem'],\n",
    "            'answer': str(int(line['answer'])),\n",
    "            'url': line['url'],\n",
    "        })\n",
    "\n",
    "# Write the updated data to JSON\n",
    "with open(\"./AMC/test.json\", mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(amc_test, json_file, indent=4, ensure_ascii=False)\n",
    "print(len(amc_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /venv/py311/lib/python3.11/site-packages (2.21.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /venv/py311/lib/python3.11/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/py311/lib/python3.11/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /venv/py311/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /venv/py311/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/py311/lib/python3.11/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/py311/lib/python3.11/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /venv/py311/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /venv/py311/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/py311/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /venv/py311/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /venv/py311/lib/python3.11/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /venv/py311/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/py311/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/py311/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /venv/py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/py311/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /venv/py311/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /venv/py311/lib/python3.11/site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/py311/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/py311/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/py311/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/py311/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/py311/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/py311/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/py311/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/py311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.21.0\n",
      "    Uninstalling datasets-2.21.0:\n",
      "      Successfully uninstalled datasets-2.21.0\n",
      "Successfully installed datasets-4.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from huggingface_hub import create_repo\n",
    "\n",
    "# After you've saved the JSON file and have the amc_test list\n",
    "# Create a Hugging Face dataset from your list\n",
    "dataset = Dataset.from_list(amc_test)\n",
    "\n",
    "# Create the repository (if it doesn't exist)\n",
    "repo_id = \"talzoomanzoo/amc_test\"\n",
    "create_repo(repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "# Push to hub\n",
    "dataset.push_to_hub(repo_id, commit_message=\"Upload AMC test dataset\")\n",
    "\n",
    "print(f\"Dataset pushed to: https://huggingface.co/datasets/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AIME/train.json...\n",
      "Processing AIME/test.json...\n",
      "Processing BRUMO/test.json...\n",
      "Processing HMMT/test.json...\n",
      "Processing MINERVA_MATH/test.json...\n",
      "Processing OLYMPIADBENCH/test.json...\n",
      "\n",
      "Total problems compiled: 2643\n",
      "\n",
      "Dataset breakdown:\n",
      "  AIME: 963 problems\n",
      "  BRUMO: 30 problems\n",
      "  HMMT: 30 problems\n",
      "  MINERVA_MATH: 272 problems\n",
      "  OLYMPIADBENCH: 1348 problems\n",
      "\n",
      "Compiled dataset saved to: compiled_math_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Compile all math datasets into one\n",
    "def compile_all_math_datasets():\n",
    "    \"\"\"\n",
    "    Compiles all math datasets from subdirectories into a single JSON file.\n",
    "    Extracts Question and answer fields from each dataset.\n",
    "    \"\"\"\n",
    "    data_dir = Path(\".\")  # Current directory\n",
    "    compiled_data = []\n",
    "    \n",
    "    # Define all dataset directories to process\n",
    "    datasets = {\n",
    "        \"AIME\": [\"train.json\", \"test.json\"],\n",
    "        \"BRUMO\": [\"test.json\"],\n",
    "        \"HMMT\": [\"test.json\"],\n",
    "        \"MINERVA_MATH\": [\"test.json\"],\n",
    "        \"OLYMPIADBENCH\": [\"test.json\"]\n",
    "    }\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_name, files in datasets.items():\n",
    "        dataset_path = data_dir / dataset_name\n",
    "        if not dataset_path.exists():\n",
    "            print(f\"Warning: {dataset_name} directory not found, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        for file_name in files:\n",
    "            file_path = dataset_path / file_name\n",
    "            if not file_path.exists():\n",
    "                print(f\"Warning: {file_path} not found, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing {dataset_name}/{file_name}...\")\n",
    "            \n",
    "            # Load the dataset file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Extract Question and answer from each entry\n",
    "            for item in data:\n",
    "                question = item.get(\"Question\", \"\")\n",
    "                answer = item.get(\"answer\", \"\")\n",
    "                \n",
    "                # Convert answer to string if it's a list (e.g., OLYMPIADBENCH)\n",
    "                if isinstance(answer, list):\n",
    "                    answer = answer[0] if len(answer) > 0 else \"\"\n",
    "                \n",
    "                compiled_entry = {\n",
    "                    \"Question\": question,\n",
    "                    \"answer\": str(answer),\n",
    "                    \"source\": dataset_name,\n",
    "                    \"split\": file_name.replace(\".json\", \"\")\n",
    "                }\n",
    "                compiled_data.append(compiled_entry)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nTotal problems compiled: {len(compiled_data)}\")\n",
    "    print(\"\\nDataset breakdown:\")\n",
    "    source_counts = Counter([item[\"source\"] for item in compiled_data])\n",
    "    for source, count in sorted(source_counts.items()):\n",
    "        print(f\"  {source}: {count} problems\")\n",
    "    \n",
    "    # Save compiled dataset\n",
    "    output_file = data_dir / \"compiled_math_dataset.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(compiled_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nCompiled dataset saved to: {output_file}\")\n",
    "    return compiled_data\n",
    "\n",
    "# Run the compilation\n",
    "compiled_dataset = compile_all_math_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:00<00:00, 638.86ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████|  443kB /  443kB,   ???B/s  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████|  443kB /  443kB,  0.00B/s  \n",
      "New Data Upload                         : |          |  0.00B /  0.00B,  0.00B/s  \n",
      "                                        : 100%|██████████|  443kB /  443kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.12s/ shards]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset pushed to: https://huggingface.co/datasets/talzoomanzoo/math_compilation\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from huggingface_hub import create_repo\n",
    "import json\n",
    "\n",
    "# Load the compiled dataset\n",
    "with open(\"compiled_math_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    compiled_math = json.load(f)\n",
    "\n",
    "# Create a Hugging Face dataset from the list\n",
    "dataset = Dataset.from_list(compiled_math)\n",
    "\n",
    "# Create the repository (if it doesn't exist)\n",
    "repo_id = \"talzoomanzoo/math_compilation\"\n",
    "create_repo(repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "# Push to hub\n",
    "dataset.push_to_hub(repo_id, commit_message=\"Upload compiled math dataset with AIME, BRUMO, HMMT, MINERVA_MATH, and OLYMPIADBENCH\")\n",
    "\n",
    "print(f\"Dataset pushed to: https://huggingface.co/datasets/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from huggingface_hub import create_repo\n",
    "import json\n",
    "\n",
    "with open(\"dataset_collection_v5.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    target_math = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(target_math)\n",
    "\n",
    "repo_id = \"talzoomanzoo/target_math\"\n",
    "create_repo(repo_id, repo_type=\"dataset\", exist_ok=True, private=True)\n",
    "\n",
    "dataset.push_to_hub(repo_id, commit_message=\"Upload target math dataset\")\n",
    "\n",
    "print(f\"Dataset pushed to: https://huggingface.co/datasets/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uid_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
